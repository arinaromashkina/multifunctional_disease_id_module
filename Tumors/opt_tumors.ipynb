{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T13:37:25.395219600Z",
          "start_time": "2024-01-07T13:26:33.310964600Z"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "38ef135d-1d43-461d-d9ea-bd6ebd09c541"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-01-07 16:26:33,314] A new study created in memory with name: no-name-96c11516-1f02-4da6-8186-3e1c0ad92236\n",
            "[I 2024-01-07 16:37:25,383] Trial 0 finished with value: 0.692307710647583 and parameters: {'n_layers': 2, 'optimizer': 'RMSprop', 'lr': 0.009197209479604193}. Best is trial 0 with value: 0.692307710647583.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  1\n",
            "  Number of pruned trials:  0\n",
            "  Number of complete trials:  1\n",
            "Best trial:\n",
            "  Value:  0.692307710647583\n",
            "  Params: \n",
            "    n_layers: 2\n",
            "    optimizer: RMSprop\n",
            "    lr: 0.009197209479604193\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "BATCHSIZE = 32\n",
        "CLASSES = 4\n",
        "DIR = os.getcwd()\n",
        "EPOCHS = 20\n",
        "N_TRAIN_EXAMPLES = BATCHSIZE * 15\n",
        "N_VALID_EXAMPLES = BATCHSIZE * 5\n",
        "\n",
        "\n",
        "def define_model(trial):\n",
        "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
        "    layers = []\n",
        "    in_channels = [3, 8, 16, 32]\n",
        "    out_channels = [8, 16, 32, 64]\n",
        "    dimension = [1, 8*111*111, 16*54*54, 32*26*26, 64*12*12]\n",
        "    for i in range(1, n_layers):\n",
        "        layers.append(nn.Conv2d(in_channels=in_channels[i-1], out_channels=out_channels[i-1], kernel_size=3))\n",
        "        layers.append(nn.BatchNorm2d(out_channels[i-1]))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
        "    layers.append(nn.Flatten())\n",
        "    layers.append(nn.Linear(dimension[n_layers - 1], CLASSES))\n",
        "    layers.append(nn.LogSoftmax(dim=1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    train_path = ('data_folder/Training/')\n",
        "    test_path = ('data_folder/Testing/')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    train_dataset = ImageFolder(root=train_path, transform=transform)\n",
        "    test_dataset = ImageFolder(root=test_path, transform=transform)\n",
        "    train_loader=DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    test_loader=DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Generate the model.\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "    # Generate the optimizers.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Get the FashionMNIST dataset.\n",
        "    train_dataloader, val_dataloader = get_data()\n",
        "    train_loss_log, train_acc_log, val_loss_log, val_acc_log = [], [], [], []\n",
        "    # Training of the model.\n",
        "    for epoch in range(EPOCHS):\n",
        "        # тренировка\n",
        "        train_epoch_loss, train_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.train()\n",
        "        for i, (imgs, labels) in enumerate(train_dataloader):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            y_pred = model(imgs)\n",
        "            loss = criterion(y_pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # log loss for the current epoch and the whole training history\n",
        "            train_epoch_loss = torch.cat(\n",
        "                (train_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "            )\n",
        "            train_loss_log.append(loss.cpu().data / labels.cpu().size(0))\n",
        "\n",
        "            # log accuracy for the current epoch and the whole training history\n",
        "            pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "            train_epoch_true_hits = torch.cat(\n",
        "                (\n",
        "                    train_epoch_true_hits,\n",
        "                    (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                )\n",
        "            )\n",
        "            train_acc_log.append(\n",
        "                (pred_classes == labels.cpu()).sum() / labels.cpu().shape[0]\n",
        "            )\n",
        "\n",
        "        # валидация\n",
        "        val_epoch_loss, val_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (imgs, labels) in enumerate(val_dataloader):\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "                y_pred = model(imgs)\n",
        "                loss = criterion(y_pred, labels)\n",
        "                val_epoch_loss = torch.cat(\n",
        "                    (val_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "                )\n",
        "\n",
        "                pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "                val_epoch_true_hits = torch.cat(\n",
        "                    (\n",
        "                        val_epoch_true_hits,\n",
        "                        (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        val_loss_log.append(val_epoch_loss.mean())\n",
        "        val_acc_log.append(\n",
        "            val_epoch_true_hits.sum()\n",
        "            / val_epoch_true_hits.size(0)\n",
        "            / val_dataloader.batch_size\n",
        "        )\n",
        "        accuracy = (val_epoch_true_hits.sum()\n",
        "                / val_epoch_true_hits.size(0)\n",
        "                / val_dataloader.batch_size).item()\n",
        "\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=75, timeout=6000)\n",
        "\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cb45da22e9d1d0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T06:48:07.295639500Z",
          "start_time": "2024-01-08T06:48:07.243505500Z"
        },
        "id": "33cb45da22e9d1d0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b7b2cdca940a04",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T06:48:08.477166800Z",
          "start_time": "2024-01-08T06:48:08.432617Z"
        },
        "id": "53b7b2cdca940a04"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8801f672683502f2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T14:15:52.353131600Z",
          "start_time": "2024-01-08T14:00:26.595547400Z"
        },
        "id": "8801f672683502f2",
        "outputId": "1c642079-ff40-46dd-f6aa-9beea6642fb6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-01-08 17:00:26,645] A new study created in memory with name: no-name-c0a30014-15fb-4efc-bfcc-2107aef4e53b\n",
            "[I 2024-01-08 17:05:14,966] Trial 0 finished with value: 0.7946428656578064 and parameters: {'n_layers': 3, 'optimizer': 'Adam', 'lr': 0.0007915009678028183}. Best is trial 0 with value: 0.7946428656578064.\n",
            "[I 2024-01-08 17:08:46,201] Trial 1 finished with value: 0.6696428656578064 and parameters: {'n_layers': 1, 'optimizer': 'RMSprop', 'lr': 4.3625382758855986e-05}. Best is trial 0 with value: 0.7946428656578064.\n",
            "[I 2024-01-08 17:12:24,076] Trial 2 finished with value: 0.5848214030265808 and parameters: {'n_layers': 3, 'optimizer': 'RMSprop', 'lr': 0.014219339653230799}. Best is trial 0 with value: 0.7946428656578064.\n",
            "[I 2024-01-08 17:15:52,311] Trial 3 finished with value: 0.4642857015132904 and parameters: {'n_layers': 2, 'optimizer': 'SGD', 'lr': 5.857129450914013e-05}. Best is trial 0 with value: 0.7946428656578064.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  4\n",
            "  Number of pruned trials:  0\n",
            "  Number of complete trials:  4\n",
            "Best trial:\n",
            "  Value:  0.7946428656578064\n",
            "  Params: \n",
            "    n_layers: 3\n",
            "    optimizer: Adam\n",
            "    lr: 0.0007915009678028183\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "BATCHSIZE = 32\n",
        "CLASSES = 4\n",
        "DIR = os.getcwd()\n",
        "EPOCHS = 10\n",
        "N_TRAIN_EXAMPLES = BATCHSIZE * 45\n",
        "N_VALID_EXAMPLES = BATCHSIZE * 7\n",
        "\n",
        "\n",
        "def define_model(trial):\n",
        "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
        "    layers = []\n",
        "    in_channels = [3, 8, 16, 32]\n",
        "    out_channels = [8, 16, 32, 64]\n",
        "    dimension = [1, 8*111*111, 16*54*54, 32*26*26, 64*12*12]\n",
        "    for i in range(n_layers):\n",
        "        layers.append(nn.Conv2d(in_channels=in_channels[i], out_channels=out_channels[i], kernel_size=3))\n",
        "        layers.append(nn.BatchNorm2d(out_channels[i]))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
        "    layers.append(nn.Flatten())\n",
        "    layers.append(nn.Linear(dimension[n_layers], CLASSES))\n",
        "    layers.append(nn.LogSoftmax(dim=1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    train_path = ('data_folder/Training/')\n",
        "    test_path = ('data_folder/Testing/')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    train_dataset = ImageFolder(root=train_path, transform=transform)\n",
        "    test_dataset = ImageFolder(root=test_path, transform=transform)\n",
        "    train_loader=DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    test_loader=DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Generate the model.\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "    # Generate the optimizers.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Get the FashionMNIST dataset.\n",
        "    train_dataloader, val_dataloader = get_data()\n",
        "    train_loss_log, train_acc_log, val_loss_log, val_acc_log = [], [], [], []\n",
        "    # Training of the model.\n",
        "    for epoch in range(EPOCHS):\n",
        "        # тренировка\n",
        "        train_epoch_loss, train_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.train()\n",
        "        for i, (imgs, labels) in enumerate(train_dataloader):\n",
        "            # Limiting training data for faster epochs.\n",
        "            if i * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
        "                break\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            y_pred = model(imgs)\n",
        "            loss = criterion(y_pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # log loss for the current epoch and the whole training history\n",
        "            train_epoch_loss = torch.cat(\n",
        "                (train_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "            )\n",
        "            train_loss_log.append(loss.cpu().data / labels.cpu().size(0))\n",
        "\n",
        "            # log accuracy for the current epoch and the whole training history\n",
        "            pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "            train_epoch_true_hits = torch.cat(\n",
        "                (\n",
        "                    train_epoch_true_hits,\n",
        "                    (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                )\n",
        "            )\n",
        "            train_acc_log.append(\n",
        "                (pred_classes == labels.cpu()).sum() / labels.cpu().shape[0]\n",
        "            )\n",
        "\n",
        "        # валидация\n",
        "        val_epoch_loss, val_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (imgs, labels) in enumerate(val_dataloader):\n",
        "                if i * BATCHSIZE >= N_VALID_EXAMPLES:\n",
        "                    break\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "                y_pred = model(imgs)\n",
        "                loss = criterion(y_pred, labels)\n",
        "                val_epoch_loss = torch.cat(\n",
        "                    (val_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "                )\n",
        "\n",
        "                pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "                val_epoch_true_hits = torch.cat(\n",
        "                    (\n",
        "                        val_epoch_true_hits,\n",
        "                        (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        val_loss_log.append(val_epoch_loss.mean())\n",
        "        val_acc_log.append(\n",
        "            val_epoch_true_hits.sum()\n",
        "            / val_epoch_true_hits.size(0)\n",
        "            / val_dataloader.batch_size\n",
        "        )\n",
        "        accuracy = (val_epoch_true_hits.sum()\n",
        "                / val_epoch_true_hits.size(0)\n",
        "                / val_dataloader.batch_size).item()\n",
        "\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=75, timeout=900)\n",
        "\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f668056dc95a9e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T14:15:52.378531700Z",
          "start_time": "2024-01-08T14:15:52.359078600Z"
        },
        "id": "f0f668056dc95a9e",
        "outputId": "6d29c534-6780-4abf-9524-b6dddf41ccce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9504d85b89dba939",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T14:38:26.119840800Z",
          "start_time": "2024-01-08T14:18:58.425668500Z"
        },
        "id": "9504d85b89dba939",
        "outputId": "dffbe3a2-f78f-48db-a5be-360fc05c17fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-01-08 17:18:58,448] A new study created in memory with name: no-name-5444bf66-0821-43c9-8c1e-0ad4693d18d9\n",
            "[I 2024-01-08 17:22:36,315] Trial 0 finished with value: 0.6022727489471436 and parameters: {'n_layers': 2, 'optimizer': 'Adam', 'lr': 0.003298320467810311}. Best is trial 0 with value: 0.6022727489471436.\n",
            "[I 2024-01-08 17:26:31,277] Trial 1 finished with value: 0.6335227489471436 and parameters: {'n_layers': 3, 'optimizer': 'Adam', 'lr': 0.03433531368025113}. Best is trial 1 with value: 0.6335227489471436.\n",
            "[I 2024-01-08 17:30:32,165] Trial 2 finished with value: 0.6505681872367859 and parameters: {'n_layers': 3, 'optimizer': 'RMSprop', 'lr': 5.9100038868915736e-05}. Best is trial 2 with value: 0.6505681872367859.\n",
            "[I 2024-01-08 17:34:34,115] Trial 3 finished with value: 0.4857954680919647 and parameters: {'n_layers': 3, 'optimizer': 'RMSprop', 'lr': 0.026466525591902382}. Best is trial 2 with value: 0.6505681872367859.\n",
            "[I 2024-01-08 17:38:26,043] Trial 4 finished with value: 0.59375 and parameters: {'n_layers': 2, 'optimizer': 'RMSprop', 'lr': 2.2884494876775724e-05}. Best is trial 2 with value: 0.6505681872367859.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  5\n",
            "  Number of pruned trials:  0\n",
            "  Number of complete trials:  5\n",
            "Best trial:\n",
            "  Value:  0.6505681872367859\n",
            "  Params: \n",
            "    n_layers: 3\n",
            "    optimizer: RMSprop\n",
            "    lr: 5.9100038868915736e-05\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "BATCHSIZE = 32\n",
        "CLASSES = 4\n",
        "DIR = os.getcwd()\n",
        "EPOCHS = 10\n",
        "N_TRAIN_EXAMPLES = BATCHSIZE * 45\n",
        "N_VALID_EXAMPLES = BATCHSIZE * 11\n",
        "\n",
        "\n",
        "def define_model(trial):\n",
        "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
        "    layers = []\n",
        "    in_channels = [3, 8, 16, 32]\n",
        "    out_channels = [8, 16, 32, 64]\n",
        "    dimension = [1, 8*111*111, 16*54*54, 32*26*26, 64*12*12]\n",
        "    for i in range(n_layers):\n",
        "        layers.append(nn.Conv2d(in_channels=in_channels[i], out_channels=out_channels[i], kernel_size=3))\n",
        "        layers.append(nn.BatchNorm2d(out_channels[i]))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
        "    layers.append(nn.Flatten())\n",
        "    layers.append(nn.Linear(dimension[n_layers], CLASSES))\n",
        "    layers.append(nn.LogSoftmax(dim=1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    train_path = ('data_folder/Training/')\n",
        "    test_path = ('data_folder/Testing/')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    train_dataset = ImageFolder(root=train_path, transform=transform)\n",
        "    test_dataset = ImageFolder(root=test_path, transform=transform)\n",
        "    train_loader=DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    test_loader=DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Generate the model.\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "    # Generate the optimizers.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Get the FashionMNIST dataset.\n",
        "    train_dataloader, val_dataloader = get_data()\n",
        "    train_loss_log, train_acc_log, val_loss_log, val_acc_log = [], [], [], []\n",
        "    # Training of the model.\n",
        "    for epoch in range(EPOCHS):\n",
        "        # тренировка\n",
        "        train_epoch_loss, train_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.train()\n",
        "        for i, (imgs, labels) in enumerate(train_dataloader):\n",
        "            # Limiting training data for faster epochs.\n",
        "            if i * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
        "                break\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            y_pred = model(imgs)\n",
        "            loss = criterion(y_pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # log loss for the current epoch and the whole training history\n",
        "            train_epoch_loss = torch.cat(\n",
        "                (train_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "            )\n",
        "            train_loss_log.append(loss.cpu().data / labels.cpu().size(0))\n",
        "\n",
        "            # log accuracy for the current epoch and the whole training history\n",
        "            pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "            train_epoch_true_hits = torch.cat(\n",
        "                (\n",
        "                    train_epoch_true_hits,\n",
        "                    (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                )\n",
        "            )\n",
        "            train_acc_log.append(\n",
        "                (pred_classes == labels.cpu()).sum() / labels.cpu().shape[0]\n",
        "            )\n",
        "\n",
        "        # валидация\n",
        "        val_epoch_loss, val_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (imgs, labels) in enumerate(val_dataloader):\n",
        "                if i * BATCHSIZE >= N_VALID_EXAMPLES:\n",
        "                    break\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "                y_pred = model(imgs)\n",
        "                loss = criterion(y_pred, labels)\n",
        "                val_epoch_loss = torch.cat(\n",
        "                    (val_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "                )\n",
        "\n",
        "                pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "                val_epoch_true_hits = torch.cat(\n",
        "                    (\n",
        "                        val_epoch_true_hits,\n",
        "                        (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        val_loss_log.append(val_epoch_loss.mean())\n",
        "        val_acc_log.append(\n",
        "            val_epoch_true_hits.sum()\n",
        "            / val_epoch_true_hits.size(0)\n",
        "            / val_dataloader.batch_size\n",
        "        )\n",
        "        accuracy = (val_epoch_true_hits.sum()\n",
        "                / val_epoch_true_hits.size(0)\n",
        "                / val_dataloader.batch_size).item()\n",
        "\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=75, timeout=1000)\n",
        "\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43feea2bf0191a1f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T15:50:37.721334600Z",
          "start_time": "2024-01-08T15:23:03.323834300Z"
        },
        "id": "43feea2bf0191a1f",
        "outputId": "a48e5702-bd44-43ae-a010-39783ae3feb2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-01-08 18:23:03,366] A new study created in memory with name: no-name-03b62c43-3553-4561-a46f-99b6daf1e643\n",
            "[I 2024-01-08 18:26:10,580] Trial 0 finished with value: 0.6136363744735718 and parameters: {'n_layers': 1, 'optimizer': 'Adam', 'lr': 0.0017587555934523921}. Best is trial 0 with value: 0.6136363744735718.\n",
            "[I 2024-01-08 18:30:58,831] Trial 1 finished with value: 0.6136363744735718 and parameters: {'n_layers': 4, 'optimizer': 'Adam', 'lr': 0.01186920310504067}. Best is trial 0 with value: 0.6136363744735718.\n",
            "[I 2024-01-08 18:34:53,816] Trial 2 finished with value: 0.5397727489471436 and parameters: {'n_layers': 3, 'optimizer': 'RMSprop', 'lr': 0.004244768833274677}. Best is trial 0 with value: 0.6136363744735718.\n",
            "[I 2024-01-08 18:38:56,255] Trial 3 finished with value: 0.4119318127632141 and parameters: {'n_layers': 4, 'optimizer': 'Adam', 'lr': 1.2658563588912667e-05}. Best is trial 0 with value: 0.6136363744735718.\n",
            "[I 2024-01-08 18:42:46,200] Trial 4 finished with value: 0.6051136255264282 and parameters: {'n_layers': 3, 'optimizer': 'Adam', 'lr': 3.259165770179295e-05}. Best is trial 0 with value: 0.6136363744735718.\n",
            "[I 2024-01-08 18:43:08,789] Trial 5 pruned. \n",
            "[I 2024-01-08 18:46:46,466] Trial 6 finished with value: 0.65625 and parameters: {'n_layers': 2, 'optimizer': 'RMSprop', 'lr': 1.7151104486607e-05}. Best is trial 6 with value: 0.65625.\n",
            "[I 2024-01-08 18:49:44,599] Trial 7 finished with value: 0.6846590638160706 and parameters: {'n_layers': 1, 'optimizer': 'Adam', 'lr': 0.00011706032846641942}. Best is trial 7 with value: 0.6846590638160706.\n",
            "[I 2024-01-08 18:50:11,391] Trial 8 pruned. \n",
            "[I 2024-01-08 18:50:37,647] Trial 9 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  10\n",
            "  Number of pruned trials:  3\n",
            "  Number of complete trials:  7\n",
            "Best trial:\n",
            "  Value:  0.6846590638160706\n",
            "  Params: \n",
            "    n_layers: 1\n",
            "    optimizer: Adam\n",
            "    lr: 0.00011706032846641942\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "BATCHSIZE = 32\n",
        "CLASSES = 4\n",
        "DIR = os.getcwd()\n",
        "EPOCHS = 10\n",
        "N_TRAIN_EXAMPLES = BATCHSIZE * 45\n",
        "N_VALID_EXAMPLES = BATCHSIZE * 11\n",
        "\n",
        "\n",
        "def define_model(trial):\n",
        "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
        "    layers = []\n",
        "    in_channels = [3, 8, 16, 32]\n",
        "    out_channels = [8, 16, 32, 64]\n",
        "    dimension = [1, 8*111*111, 16*54*54, 32*26*26, 64*12*12]\n",
        "    for i in range(n_layers):\n",
        "        layers.append(nn.Conv2d(in_channels=in_channels[i], out_channels=out_channels[i], kernel_size=3))\n",
        "        layers.append(nn.BatchNorm2d(out_channels[i]))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
        "    layers.append(nn.Flatten())\n",
        "    layers.append(nn.Linear(dimension[n_layers], CLASSES))\n",
        "    layers.append(nn.LogSoftmax(dim=1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    train_path = ('data_folder/Training/')\n",
        "    test_path = ('data_folder/Testing/')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    train_dataset = ImageFolder(root=train_path, transform=transform)\n",
        "    test_dataset = ImageFolder(root=test_path, transform=transform)\n",
        "    train_loader=DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    test_loader=DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Generate the model.\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "    # Generate the optimizers.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Get the FashionMNIST dataset.\n",
        "    train_dataloader, val_dataloader = get_data()\n",
        "    train_loss_log, train_acc_log, val_loss_log, val_acc_log = [], [], [], []\n",
        "    # Training of the model.\n",
        "    for epoch in range(EPOCHS):\n",
        "        # тренировка\n",
        "        train_epoch_loss, train_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.train()\n",
        "        for i, (imgs, labels) in enumerate(train_dataloader):\n",
        "            # Limiting training data for faster epochs.\n",
        "            if i * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
        "                break\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            y_pred = model(imgs)\n",
        "            loss = criterion(y_pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # log loss for the current epoch and the whole training history\n",
        "            train_epoch_loss = torch.cat(\n",
        "                (train_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "            )\n",
        "            train_loss_log.append(loss.cpu().data / labels.cpu().size(0))\n",
        "\n",
        "            # log accuracy for the current epoch and the whole training history\n",
        "            pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "            train_epoch_true_hits = torch.cat(\n",
        "                (\n",
        "                    train_epoch_true_hits,\n",
        "                    (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                )\n",
        "            )\n",
        "            train_acc_log.append(\n",
        "                (pred_classes == labels.cpu()).sum() / labels.cpu().shape[0]\n",
        "            )\n",
        "\n",
        "        # валидация\n",
        "        val_epoch_loss, val_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (imgs, labels) in enumerate(val_dataloader):\n",
        "                if i * BATCHSIZE >= N_VALID_EXAMPLES:\n",
        "                    break\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "                y_pred = model(imgs)\n",
        "                loss = criterion(y_pred, labels)\n",
        "                val_epoch_loss = torch.cat(\n",
        "                    (val_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "                )\n",
        "\n",
        "                pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "                val_epoch_true_hits = torch.cat(\n",
        "                    (\n",
        "                        val_epoch_true_hits,\n",
        "                        (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        val_loss_log.append(val_epoch_loss.mean())\n",
        "        val_acc_log.append(\n",
        "            val_epoch_true_hits.sum()\n",
        "            / val_epoch_true_hits.size(0)\n",
        "            / val_dataloader.batch_size\n",
        "        )\n",
        "        accuracy = (val_epoch_true_hits.sum()\n",
        "                / val_epoch_true_hits.size(0)\n",
        "                / val_dataloader.batch_size).item()\n",
        "\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=10, timeout=2000)\n",
        "\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba3d76143321ea4d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-08T16:34:57.865165600Z",
          "start_time": "2024-01-08T15:59:47.103929200Z"
        },
        "id": "ba3d76143321ea4d",
        "outputId": "f02d06da-525d-42cd-86a4-f8f41b3f7252"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-01-08 18:59:47,193] A new study created in memory with name: no-name-8022807d-237c-4adc-9022-1d5d672768b3\n",
            "[I 2024-01-08 19:03:48,214] Trial 0 finished with value: 0.6818181872367859 and parameters: {'n_layers': 3, 'optimizer': 'Adam', 'lr': 7.938652330546161e-05}. Best is trial 0 with value: 0.6818181872367859.\n",
            "[I 2024-01-08 19:06:36,016] Trial 1 finished with value: 0.2954545319080353 and parameters: {'n_layers': 1, 'optimizer': 'SGD', 'lr': 0.027824206659105427}. Best is trial 0 with value: 0.6818181872367859.\n",
            "[I 2024-01-08 19:10:52,705] Trial 2 finished with value: 0.40340909361839294 and parameters: {'n_layers': 4, 'optimizer': 'SGD', 'lr': 0.0003108239833523926}. Best is trial 0 with value: 0.6818181872367859.\n",
            "[I 2024-01-08 19:13:33,914] Trial 3 finished with value: 0.6931818127632141 and parameters: {'n_layers': 1, 'optimizer': 'RMSprop', 'lr': 5.4321047942608654e-05}. Best is trial 3 with value: 0.6931818127632141.\n",
            "[I 2024-01-08 19:17:43,168] Trial 4 finished with value: 0.6590909361839294 and parameters: {'n_layers': 4, 'optimizer': 'RMSprop', 'lr': 0.00010936184528812765}. Best is trial 3 with value: 0.6931818127632141.\n",
            "[I 2024-01-08 19:18:09,017] Trial 5 pruned. \n",
            "[I 2024-01-08 19:18:33,762] Trial 6 pruned. \n",
            "[I 2024-01-08 19:19:23,065] Trial 7 pruned. \n",
            "[I 2024-01-08 19:23:15,652] Trial 8 finished with value: 0.7357954382896423 and parameters: {'n_layers': 3, 'optimizer': 'Adam', 'lr': 0.0009974157877713538}. Best is trial 8 with value: 0.7357954382896423.\n",
            "[I 2024-01-08 19:24:03,490] Trial 9 pruned. \n",
            "[I 2024-01-08 19:24:43,949] Trial 10 pruned. \n",
            "[I 2024-01-08 19:27:20,993] Trial 11 finished with value: 0.6988636255264282 and parameters: {'n_layers': 1, 'optimizer': 'Adam', 'lr': 0.004812568303262069}. Best is trial 8 with value: 0.7357954382896423.\n",
            "[I 2024-01-08 19:27:43,323] Trial 12 pruned. \n",
            "[I 2024-01-08 19:28:03,483] Trial 13 pruned. \n",
            "[I 2024-01-08 19:28:25,809] Trial 14 pruned. \n",
            "[I 2024-01-08 19:28:41,585] Trial 15 pruned. \n",
            "[I 2024-01-08 19:34:57,534] Trial 16 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  17\n",
            "  Number of pruned trials:  10\n",
            "  Number of complete trials:  7\n",
            "Best trial:\n",
            "  Value:  0.7357954382896423\n",
            "  Params: \n",
            "    n_layers: 3\n",
            "    optimizer: Adam\n",
            "    lr: 0.0009974157877713538\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "BATCHSIZE = 32\n",
        "CLASSES = 4\n",
        "DIR = os.getcwd()\n",
        "EPOCHS = 10\n",
        "N_TRAIN_EXAMPLES = BATCHSIZE * 45\n",
        "N_VALID_EXAMPLES = BATCHSIZE * 11\n",
        "\n",
        "\n",
        "def define_model(trial):\n",
        "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
        "    layers = []\n",
        "    in_channels = [3, 8, 16, 32]\n",
        "    out_channels = [8, 16, 32, 64]\n",
        "    dimension = [1, 8 * 111 * 111, 16 * 54 * 54, 32 * 26 * 26, 64 * 12 * 12]\n",
        "    for i in range(n_layers):\n",
        "        layers.append(nn.Conv2d(in_channels=in_channels[i], out_channels=out_channels[i], kernel_size=3))\n",
        "        layers.append(nn.BatchNorm2d(out_channels[i]))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2))\n",
        "    layers.append(nn.Flatten())\n",
        "    layers.append(nn.Linear(dimension[n_layers], CLASSES))\n",
        "    layers.append(nn.LogSoftmax(dim=1))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    train_path = ('data_folder/Training/')\n",
        "    test_path = ('data_folder/Testing/')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    train_dataset = ImageFolder(root=train_path, transform=transform)\n",
        "    test_dataset = ImageFolder(root=test_path, transform=transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32, shuffle=True,\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Generate the model.\n",
        "    model = define_model(trial).to(DEVICE)\n",
        "    # Generate the optimizers.\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Get the FashionMNIST dataset.\n",
        "    train_dataloader, val_dataloader = get_data()\n",
        "    train_loss_log, train_acc_log, val_loss_log, val_acc_log = [], [], [], []\n",
        "    # Training of the model.\n",
        "    for epoch in range(EPOCHS):\n",
        "        # тренировка\n",
        "        train_epoch_loss, train_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.train()\n",
        "        for i, (imgs, labels) in enumerate(train_dataloader):\n",
        "            # Limiting training data for faster epochs.\n",
        "            if i * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
        "                break\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            y_pred = model(imgs)\n",
        "            loss = criterion(y_pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # log loss for the current epoch and the whole training history\n",
        "            train_epoch_loss = torch.cat(\n",
        "                (train_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "            )\n",
        "            train_loss_log.append(loss.cpu().data / labels.cpu().size(0))\n",
        "\n",
        "            # log accuracy for the current epoch and the whole training history\n",
        "            pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "            train_epoch_true_hits = torch.cat(\n",
        "                (\n",
        "                    train_epoch_true_hits,\n",
        "                    (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                )\n",
        "            )\n",
        "            train_acc_log.append(\n",
        "                (pred_classes == labels.cpu()).sum() / labels.cpu().shape[0]\n",
        "            )\n",
        "\n",
        "        # валидация\n",
        "        val_epoch_loss, val_epoch_true_hits = torch.empty(0), torch.empty(0)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (imgs, labels) in enumerate(val_dataloader):\n",
        "                if i * BATCHSIZE >= N_VALID_EXAMPLES:\n",
        "                    break\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "                y_pred = model(imgs)\n",
        "                loss = criterion(y_pred, labels)\n",
        "                val_epoch_loss = torch.cat(\n",
        "                    (val_epoch_loss, loss.cpu().unsqueeze(0) / labels.cpu().size(0))\n",
        "                )\n",
        "\n",
        "                pred_classes = torch.argmax(y_pred.cpu(), dim=-1)\n",
        "                val_epoch_true_hits = torch.cat(\n",
        "                    (\n",
        "                        val_epoch_true_hits,\n",
        "                        (pred_classes == labels.cpu()).sum().unsqueeze(0),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        val_loss_log.append(val_epoch_loss.mean())\n",
        "        val_acc_log.append(\n",
        "            val_epoch_true_hits.sum()\n",
        "            / val_epoch_true_hits.size(0)\n",
        "            / val_dataloader.batch_size\n",
        "        )\n",
        "        accuracy = (val_epoch_true_hits.sum()\n",
        "                    / val_epoch_true_hits.size(0)\n",
        "                    / val_dataloader.batch_size).item()\n",
        "\n",
        "        trial.report(accuracy, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=20, timeout=2000)\n",
        "\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a964aee157f262",
      "metadata": {
        "id": "42a964aee157f262"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}